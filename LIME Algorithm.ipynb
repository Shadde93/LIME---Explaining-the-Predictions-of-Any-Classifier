{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble, sklearn.metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_text(text):\n",
    "    \"\"\"\n",
    "    Sanitizes text by\n",
    "    - converting all text to lowercase\n",
    "    - removing newlines and replacing by space\n",
    "    - replacing everything that is not a-z or apostrophe or space to a space\n",
    "    \"\"\"\n",
    "    sanitized_text = text.lower()\n",
    "    \n",
    "    sanitized_text = re.sub(r\"\\n\",\" \", sanitized_text)\n",
    "    sanitized_text = re.sub(r\"[^a-z']+\",\" \", sanitized_text)\n",
    "    \n",
    "    return sanitized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_kernel(x, z, sigma=1, distance=cosine_similarity):\n",
    "    \"\"\" The data-point distance kernel as described in the paper \"\"\"\n",
    "    return np.exp( -np.power(distance(x,z),2)/np.power(sigma,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_around(x_text_vec):\n",
    "    \"\"\" \n",
    "    Method for sampling an instance that is in the vicinity\n",
    "    of x_text_vec as described by the paper \n",
    "    \"\"\"\n",
    "    x_text_vec = np.array(x_text_vec, copy=True)  \n",
    "\n",
    "    max_samples = x_text_vec.shape[1]\n",
    "    num_samples = np.random.randint(max_samples, size=1)\n",
    "    indices = np.random.choice(range(max_samples), num_samples, replace=False)\n",
    "\n",
    "    x_text_vec[:,~indices] = 0\n",
    "    return x_text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_lasso_path(features, labels, weights, num_features=10):\n",
    "    \"\"\"\n",
    "    Feature selection using lasso path, taken from the authors \n",
    "    github(https://github.com/marcotcr/lime) page and slightly modified.\n",
    "    \"\"\"\n",
    "    labels = labels[:,0] #we use the first probability\n",
    "    weighted_data = np.multiply(features - np.average(features, axis=0, weights=weights), \n",
    "                                  np.sqrt(weights[:, np.newaxis])) #weights[:, np.newaxis]=(d,)->(d,1) conversion\n",
    "    weighted_labels = np.multiply(labels - np.average(labels, weights=weights), \n",
    "                                  np.sqrt(weights))\n",
    "    \n",
    "    _,_,coefs = linear_model.lars_path(weighted_data,\n",
    "                                 weighted_labels,\n",
    "                                 method='lasso',\n",
    "                                 verbose=False)\n",
    "\n",
    "    nonzero = range(weighted_data.shape[1])\n",
    "    for i in range(len(coefs.T) - 1, 0, -1):\n",
    "        nonzero = coefs.T[i].nonzero()[0]\n",
    "        if len(nonzero) <= num_features:\n",
    "            break\n",
    "    used_features = nonzero\n",
    "    return used_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_linear_expl_LIME(x_instance, num_samples, clf, sim_kernel, expl_length):\n",
    "    \"\"\"\n",
    "    Trains a linear explanator around a vicinity of a data point, in this case, a text\n",
    "    x_instance: a text to be classified and then explained.\n",
    "    \n",
    "    \"\"\"\n",
    "    sim_kernel_x = lambda z: sim_kernel(x_text_vec, z)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    x_text_vec = mlb.fit_transform([x_instance.split(\" \")])\n",
    "    \n",
    "    Z_features = []\n",
    "    Z_labels = []\n",
    "    Z_sim_kernel = []\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        z_text_vec = sample_around(x_text_vec)\n",
    "        z_text = \" \".join(mlb.inverse_transform(z_text_vec)[0])\n",
    "        Z_features.append( z_text_vec )\n",
    "\n",
    "        Z_labels.append( clf.predict_proba([z_text])[0][1] )\n",
    "        Z_sim_kernel.append( sim_kernel_x(z_text_vec) )\n",
    "        \n",
    "    local_train_X = np.vstack(Z_features)\n",
    "    local_train_y = np.vstack(Z_labels)\n",
    "    weights = np.hstack(Z_sim_kernel)[0]\n",
    "\n",
    "    selected_features = feature_selection_lasso_path(local_train_X, local_train_y, weights, expl_length)\n",
    "\n",
    "    explanator_clf = linear_model.Ridge(alpha=1, fit_intercept=True)\n",
    "    explanator_clf.fit(local_train_X[:, selected_features], local_train_y)\n",
    "    \n",
    "    return explanator_clf.coef_, mlb.classes_[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data and train black box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cat = ['alt.atheism', 'soc.religion.christian']\n",
    "\n",
    "# Used neat Sklearn method for both fetching and splitting the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=selected_cat, shuffle=True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=selected_cat, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_index = 15 # \"Randomly\" selected instance\n",
    "x_instance_unsanitized = newsgroups_test.data[instance_index]\n",
    "x_instance_sanitized = sanitize_text(x_instance_unsanitized)\n",
    "\n",
    "x_instance_target = newsgroups_test.target[instance_index]\n",
    "\n",
    "for enum, text in enumerate(newsgroups_train.data):\n",
    "    newsgroups_train.data[enum] = sanitize_text(text)\n",
    "    \n",
    "for enum, text in enumerate(newsgroups_test.data):\n",
    "    newsgroups_test.data[enum] = sanitize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data by feature extraction using TF-FID transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text data for black box supervised training.\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train black box on transformed text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_box = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "black_box.fit(train_vectors, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177288528389339"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = black_box.predict(test_vectors)\n",
    "sklearn.metrics.roc_auc_score(newsgroups_test.target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = make_pipeline(vectorizer, black_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LIME implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848cea12a2f44f019eab9f559b0e9b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefs, selected_words = sparse_linear_expl_LIME(x_instance_sanitized, 5000, model_pipeline, similarity_kernel, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: nyeda@cnsvax.uwec.edu (David Nye)\n",
      "Subject: College atheists\n",
      "Organization: University of Wisconsin Eau Claire\n",
      "Lines: 10\n",
      "\n",
      "I read an article about a poll done of students at the Ivy League\n",
      "schools in which it was reported that a third of the students\n",
      "indentified themselves as atheists.  This is a lot higher than among the\n",
      "general population.  I wonder what the reasons for this discrepancy are?\n",
      "Is it because they are more intelligent?  Younger?  Is this the wave of\n",
      "the future?\n",
      " \n",
      "David Nye (nyeda@cnsvax.uwec.edu).  Midelfort Clinic, Eau Claire WI\n",
      "This is patently absurd; but whoever wishes to become a philosopher\n",
      "must learn not to be frightened by absurdities. -- Bertrand Russell\n",
      "\n",
      "\n",
      "\n",
      "Most explanatory features: ['from' 'edu' 'than' 'more' 'article' 'atheists']\n",
      "Most explanatory feature coeffs: [ 0.07330597 -0.02757331 -0.03154749 -0.03592764 -0.06914742 -0.07753285]\n"
     ]
    }
   ],
   "source": [
    "print(x_instance_unsanitized)\n",
    "print(\"\\n\\nMost explanatory features: {}\".format(selected_words[coefs[0].argsort()][::-1]))\n",
    "print(\"Most explanatory feature coeffs: {}\".format(coefs[0][coefs[0].argsort()][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black box model prediction: [[0.346 0.654]]\n",
      "Correct label: 0\n"
     ]
    }
   ],
   "source": [
    "black_box_pred = model_pipeline.predict_proba([x_instance_sanitized])\n",
    "print(\"Black box model prediction: {}\".format(black_box_pred))\n",
    "print(\"Correct label: {}\".format(x_instance_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see if the explanator is correct by investigating removal of \"explanatory\" features!\n",
    "#### If the coefficients are positive, the should decrease the probability of the positive class, if they are negative they should decrease the probability of the negative class ( zero in our case )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:746: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black box model prediction after feature removal: [[0.29 0.71]]\n",
      "Correct label: 0\n"
     ]
    }
   ],
   "source": [
    "x_instance_vec = vectorizer.transform([x_instance_sanitized]).copy()\n",
    "x_instance_vec[0, vectorizer.vocabulary_['atheists']] = 0\n",
    "x_instance_vec[0, vectorizer.vocabulary_['keith']] = 0\n",
    "new_prob = black_box.predict_proba(x_instance_vec)\n",
    "print(\"Black box model prediction after feature removal: {}\".format(new_prob))\n",
    "print(\"Correct label: {}\".format(x_instance_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to author, this removal should move the prediction towards the opposite class by the sum of all weights corresponding to the removed features, we can see this behavior above, this means our explanator model successfully learned how our black box model behaves in the vicinity of the data point, in our case x_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see if we can apply this on a new dataset, and try to trust our new model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cat2 = ['rec.sport.baseball','rec.sport.hockey']\n",
    "newsgroups_train2 = fetch_20newsgroups(subset='train', categories=selected_cat2, shuffle=False)\n",
    "newsgroups_test2 = fetch_20newsgroups(subset='test', categories=selected_cat2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_index2 = 17 # \"Randomly\" selected instances for report: 17, 54, and 82\n",
    "x_instance_unsanitized2 = newsgroups_test2.data[instance_index2]\n",
    "x_instance_sanitized2 = sanitize_text(x_instance_unsanitized2)\n",
    "\n",
    "\n",
    "x_instance_target2 = newsgroups_test2.target[instance_index2]\n",
    "\n",
    "for enum, text in enumerate(newsgroups_train2.data):\n",
    "    newsgroups_train2.data[enum] = sanitize_text(text)\n",
    "    \n",
    "for enum, text in enumerate(newsgroups_test2.data):\n",
    "    newsgroups_test2.data[enum] = sanitize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: yatrou@INRS-Telecom.Uquebec.CA (Paul Yatrou)\n",
      "Subject: Re: Stewart homered the Wings!!\n",
      "Organization: Bell-Northern Research Montreal, Canada.\n",
      "Lines: 25\n",
      "\n",
      "In <andy.bgsu.edu-250493225109@m248-100.bgsu.edu> andy.bgsu.edu (Ryan ) writes:\n",
      "\n",
      ">  Paul Stewart called *the* single worst game I've seen this year.\n",
      "> Federov's major was obvious, and I don't dispute it.  \n",
      ">However, Chaisson's penalty shouldn't even have been a penalty, let alone\n",
      "> a major and a game misconduct.\n",
      ">\n",
      "\n",
      "I don't \"notice\" refs and linesmen until the playoffs come around, and\n",
      "yes I have to agree that Stewart called the *two* worst games I've seen\n",
      "so far (Mtl-Quebec game 1, and last nights Toronto-Detroit game).\n",
      "\n",
      "What's the scoop on this guy? Is he the latest incarnation of\n",
      "KERRY FRASER??? Just because you are boneheadedly stubborn doesn't\n",
      "make you a good ref!!! Making the right call does...\n",
      "\n",
      "My votes for:\n",
      "Best Ref: Van Hellemond\n",
      "Most Improved: Koharski\n",
      "Worst: Paul Stewart\n",
      "\n",
      "(Oops, I don't really want to start a best/worst ref thread so don't\n",
      "follow up ;-)\n",
      "\n",
      "Paul Yatrou.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_instance_unsanitized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text data for black box supervised training.\n",
    "vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors2 = vectorizer2.fit_transform(newsgroups_train2.data)\n",
    "test_vectors2 = vectorizer2.transform(newsgroups_test2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_box2 = sklearn.tree.DecisionTreeClassifier()\n",
    "black_box2.fit(train_vectors2, newsgroups_train2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hockey'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_feture = black_box2.feature_importances_.argsort()[-1]\n",
    "vectorizer2.get_feature_names()[important_feture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perf: 1.0, Test perf: 0.8718332354816513\n"
     ]
    }
   ],
   "source": [
    "pred_train = black_box2.predict(train_vectors2)\n",
    "pred_test = black_box2.predict(test_vectors2)\n",
    "train_f1 = sklearn.metrics.roc_auc_score(newsgroups_train2.target, pred_train)\n",
    "test_f1 = sklearn.metrics.roc_auc_score(newsgroups_test2.target, pred_test)\n",
    "print(\"Train perf: {}, Test perf: {}\".format(train_f1, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97671f4366a44d79a3bf3757eb91490d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_pipeline2 = make_pipeline(vectorizer2, black_box2)\n",
    "coefs2OF, selected_words2OF = sparse_linear_expl_LIME(x_instance_sanitized2, 5000, model_pipeline2, similarity_kernel, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mre@teal.Eng.Sun.COM (Mike Eisler)\n",
      "Subject: Re: LET'S GO BUFFALO!\n",
      "Organization: Sun Microsystems, Mountain View, CA  USA\n",
      "Lines: 16\n",
      "NNTP-Posting-Host: teal\n",
      "\n",
      "In article <93111.205214RAP115@psuvm.psu.edu> Robbie Po <RAP115@psuvm.psu.edu> writes:\n",
      ">In article <AfpIKNm00WBLI1isJ1@andrew.cmu.edu>, \"William K. Willis\"\n",
      "><ww1a+@andrew.cmu.edu> says:\n",
      ">>\n",
      ">>     You know, I never really appreciated them before!\n",
      ">\n",
      ">Looks like Bob Errey's ring really sparkles in that locker room, and everyone\n",
      ">else wants one, too! :-)  Correct me if I'm wrong though, (just through\n",
      "\n",
      "No, Fuhr's 5 rings out sparkle Errey's. And doesn't Bob have 2 rings?\n",
      "-- \n",
      "Mike Eisler, mre@Eng.Sun.Com  ``Not only are they [Leafs] the best team, but\n",
      " their fans are even more intelligent and insightful than Pittsburgh's.  Their\n",
      " players are mighty bright, too.  I mean, he really *was* going to get his\n",
      " wallet back, right?'' Jan Brittenson 3/93, on Leaf/Pen woofers in\n",
      " rec.sport.hockey\n",
      "\n",
      "\n",
      "\n",
      "Most explanatory features: ['hockey' 'ca' \"pittsburgh's\" 'go' 'and' 'the']\n",
      "Most explanatory feature coeffs: [ 0.2356803   0.2105153   0.19596321  0.1458953   0.03311538 -0.04338921]\n"
     ]
    }
   ],
   "source": [
    "print(x_instance_unsanitized2)\n",
    "print(\"\\n\\nMost explanatory features: {}\".format(selected_words2OF[coefs2OF[0].argsort()][::-1]))\n",
    "print(\"Most explanatory feature coeffs: {}\".format(coefs2OF[0][coefs2OF[0].argsort()][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black box model prediction: [[0. 1.]]\n",
      "Correct label: 1=rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "black_box_pred = model_pipeline2.predict_proba([x_instance_sanitized2])\n",
    "print(\"Black box model prediction: {}\".format(black_box_pred))\n",
    "print(\"Correct label: {}={}\".format(x_instance_target2, selected_cat2[x_instance_target2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets compare this to hyperparameter fitted black box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Lets find some good parameters using random search bith k-fold cross validation!\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100]#[int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap\n",
    "              }\n",
    "\n",
    "\n",
    "black_box_tofit = sklearn.ensemble.RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = black_box_tofit, param_distributions = random_grid, n_iter=100, cv = 2, verbose=2)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_vectors2, newsgroups_train2.target)\n",
    "black_box_fitted = rf_random.best_estimator_\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=110, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=4, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The resuling model from the random search k-fold cross validation\n",
    "black_box_fitted = sklearn.ensemble.RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "            max_depth=110, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=4, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "black_box_fitted.fit(train_vectors2, newsgroups_train2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perf: 0.9891624790619765, Test perf: 0.9309703730358642\n"
     ]
    }
   ],
   "source": [
    "pred_train = black_box_fitted.predict(train_vectors2)\n",
    "pred_test = black_box_fitted.predict(test_vectors2)\n",
    "train_f1 = sklearn.metrics.roc_auc_score(newsgroups_train2.target, pred_train)\n",
    "test_f1 = sklearn.metrics.roc_auc_score(newsgroups_test2.target, pred_test)\n",
    "print(\"Train perf: {}, Test perf: {}\".format(train_f1, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ee2c6e8ddd4e5f955d50ceb2049c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_pipeline2 = make_pipeline(vectorizer2, black_box_fitted)\n",
    "coefs2, selected_words2 = sparse_linear_expl_LIME(x_instance_sanitized2, 5000, model_pipeline2, similarity_kernel, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Most explanatory features: ['playoffs' 'wings' 'quebec' 'toronto' 'ca' 'detroit']\n",
      "Most explanatory feature coeffs: [0.08724327 0.06841966 0.05950898 0.05446195 0.04711267 0.04399823]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nMost explanatory features: {}\".format(selected_words2[coefs2[0].argsort()][::-1]))\n",
    "print(\"Most explanatory feature coeffs: {}\".format(coefs2[0][coefs2[0].argsort()][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black box model prediction: [[0.25190144 0.74809856]]\n",
      "Correct label: 1=rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "black_box_pred = model_pipeline2.predict_proba([x_instance_sanitized2])\n",
    "print(\"Black box model prediction: {}\".format(black_box_pred))\n",
    "print(\"Correct label: {}={}\".format(x_instance_target2, selected_cat2[x_instance_target2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
